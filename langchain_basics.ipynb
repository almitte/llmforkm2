{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ein Master of Laws (LLM) ist ein postgraduales Studium für Juristen, die bereits einen Abschluss in Jura haben. Es ermöglicht den Studierenden, sich auf ein spezifisches Rechtsgebiet zu spezialisieren oder ihre Kenntnisse in einem bestimmten Bereich zu vertiefen. Ein LLM kann in verschiedenen Ländern angeboten werden und wird oft von internationalen Studierenden gewählt, um ihr Wissen im internationalen Recht zu erweitern.', response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 18, 'total_tokens': 126}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a8d842e-c280-4adc-bdbb-c8fb5f5e2b61-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Erkläre LLMs in drei Sätzen\")\n",
    "# returns JSON data with model output and metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ein LLM (Master of Laws) ist ein postgraduales Studium für Juristen, um ihre Kenntnisse auf einem bestimmten Rechtsgebiet zu vertiefen. LLM-Programme werden in vielen Ländern weltweit angeboten und können sowohl allgemeine Rechtspraxis als auch spezialisierte Bereiche abdecken. Absolventen eines LLM-Programms haben in der Regel bessere Karrierechancen und können in verschiedenen Bereichen des Rechts tätig werden.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"Erkläre LLMs in drei Sätzen\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs, oder Language Model Models, sind eine Art von künstlicher Intelligenz, die dazu entwickelt wurden, natürlich-sprachliche Daten zu verstehen und zu generieren. Sie werden häufig für Aufgaben wie Übersetzung, Textgenerierung und Textklassifizierung verwendet. LLMs basieren oft auf neuronalen Netzwerken und werden durch umfangreiche Trainingsdatensätze aufgebaut, um eine hohe Genauigkeit und Leistung zu erzielen.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser() # Initialisierung eines Objekts zum Extrahieren des Strings \n",
    "chain = model | parser # Output des Model wird in parser Objekt gepiped\n",
    "chain.invoke(\"Erkläre LLMs in drei Sätzen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Beantworte die Frage basierend auf dem unten gegebenen Kontext. \n",
    "Wenn das Beantworten der Frage nicht möglich ist durch den gegebenen Kontekt, antworte \"Ich weiß es nicht\". \n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template) # Template Prompt Objekt erzeugen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich weiß es nicht.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt.format(context=\"Es gibt auschließlich weiße Schwäne.\", question = \"Gibt es schwarze Schwäne?\")).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich weiß es nicht.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Es gibt ausschließlich weiße Schwäne.\", \n",
    "    \"question\": \"Gibt es schwarze Schwäne?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sì, in Australia ci sono cigni neri.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "diff_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Übersetze {answer} in {language}\"\n",
    ")\n",
    "\n",
    "diff_chain = (\n",
    "    # dic um Input zu definieren, dies wird dann in diff_prompt gepiped\n",
    "    {\"answer\": chain, # Antwort wird aus der anderen Chain als Input genommen\n",
    "     \"language\": itemgetter(\"language\")} # Sprache aus Dictonary\n",
    "    | diff_prompt | model | parser \n",
    ")\n",
    "\n",
    "diff_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Alle Schwäne sind weiß, außer in Australien. Dort gibt es schwarze.\",\n",
    "        \"question\": \"Gibt es schwarze Schwäne?\", \n",
    "        \"language\": \"Italienisch\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[0.00901121263534296, -0.03669295798076368, -0.00873078020641617, -0.025039455366582045, -0.01990443465229751, 0.006879930087054519, -0.0007376917735520665, -0.012351468412199479, -0.009098458010849193, -0.04162856202435205]\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Gibt es schwarze Schwäne?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\") # Dimension der Embedding Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = embeddings.embed_query(\"Es gibt in Europa nur weiße Schwäne\")\n",
    "sen2 = embeddings.embed_query(\"Es gibt schwarze Schwäne in Australien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8740940461805541, 0.9229002354973087)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sen1_sim = cosine_similarity([embedded_query], [sen1])[0][0]\n",
    "query_sen2_sim = cosine_similarity([embedded_query], [sen2])[0][0]\n",
    "query_sen1_sim, query_sen2_sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
