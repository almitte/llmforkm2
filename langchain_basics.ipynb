{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ja, ich bin ein Modell von OpenAI und basiere auf der Technologie GPT-3. Ich bin ein KI-gesteuerter Chatbot.', response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 17, 'total_tokens': 51}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6d14c275-5ae8-47aa-b098-af97da169ef9-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Bist du nicht chat gpt 3?\")\n",
    "# returns JSON data with model output and metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs, kurz für \"Large Language Models\", sind fortgeschrittene KI-Systeme, die darauf trainiert sind, menschliche Sprache zu verstehen und zu generieren. Sie basieren auf einer Architektur namens Transformer, die große Mengen an Textdaten verarbeitet, um Muster, Kontext und Sprachnuancen zu lernen. Diese Modelle werden in vielfältigen Anwendungen genutzt, von der Textgenerierung und Übersetzung bis hin zur Beantwortung von Fragen und der Unterstützung bei der Textbearbeitung, indem sie fähig sind, kohärente und kontextbezogene Antworten zu generieren.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"Erkläre LLMs in drei Sätzen\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs, oder \"Large Language Models\", sind fortschrittliche KI-Systeme, die darauf trainiert sind, menschliche Sprache zu verstehen und zu generieren. Sie basieren auf neuronalen Netzwerken, insbesondere auf Architekturen wie Transformer, die große Mengen an Textdaten verarbeiten, um Muster und Kontexte zu lernen. LLMs werden in verschiedenen Anwendungen verwendet, darunter Textgenerierung, Übersetzung, Zusammenfassung und andere sprachbasierte Aufgaben, um Benutzern zu helfen, Informationen effizienter zu verarbeiten und zu verstehen.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser() # Initialisierung eines Objekts zum Extrahieren des Strings \n",
    "chain = model | parser # Output des Model wird in parser Objekt gepiped\n",
    "chain.invoke(\"Erkläre LLMs in drei Sätzen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Beantworte die Frage basierend auf dem unten gegebenen Kontext. \n",
    "Wenn das Beantworten der Frage nicht möglich ist durch den gegebenen Kontekt, antworte \"Ich weiß es nicht\". \n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template) # Template Prompt Objekt erzeugen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich weiß es nicht.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt.format(context=\"Es gibt auschließlich weiße Schwäne.\", question = \"Gibt es schwarze Schwäne?\")).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Du heißt Max.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich weiß es nicht.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Es gibt ausschließlich weiße Schwäne.\", \n",
    "    \"question\": \"Gibt es schwarze Schwäne?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Das Wort \"Ja\" auf Italienisch ist \"Sì\".'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "diff_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Übersetze {answer} in {language}\"\n",
    ")\n",
    "\n",
    "diff_chain = (\n",
    "    # dic um Input zu definieren, dies wird dann in diff_prompt gepiped\n",
    "    {\"answer\": chain, # Antwort wird aus der anderen Chain als Input genommen\n",
    "     \"language\": itemgetter(\"language\")} # Sprache aus Dictonary\n",
    "    | diff_prompt | model | parser \n",
    ")\n",
    "\n",
    "diff_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Alle Schwäne sind weiß, außer in Australien. Dort gibt es schwarze.\",\n",
    "        \"question\": \"Gibt es schwarze Schwäne?\", \n",
    "        \"language\": \"Italienisch\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[0.00901121263534296, -0.03669295798076368, -0.00873078020641617, -0.025039455366582045, -0.01990443465229751, 0.006879930087054519, -0.0007376917735520665, -0.012351468412199479, -0.009098458010849193, -0.04162856202435205]\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Gibt es schwarze Schwäne?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\") # Dimension der Embedding Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = embeddings.embed_query(\"Es gibt in Europa nur weiße Schwäne\")\n",
    "sen2 = embeddings.embed_query(\"Es gibt schwarze Schwäne in Australien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8740940461805541, 0.9229002354973087)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sen1_sim = cosine_similarity([embedded_query], [sen1])[0][0]\n",
    "query_sen2_sim = cosine_similarity([embedded_query], [sen2])[0][0]\n",
    "query_sen1_sim, query_sen2_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rag\n",
    "run_id= \"25eec295-c3d5-4946-80bf-6e17f19e15af\"\n",
    "client = rag.get_client()\n",
    "feedback_record = client.create_feedback(\n",
    "    run_id,\n",
    "    key = \"test\",\n",
    "    score=1,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate \n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langsmith import Client\n",
    "from langchain.callbacks import LangChainTracer\n",
    "\n",
    "# load .env Variablen \n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# tracing mit Langsmith from Langchain\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-project\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_feedback(run_id,score):\n",
    "    key =f\"feedback_{run_id}\"\n",
    "    client.create_feedback(\n",
    "        run_id,\n",
    "        key=key,\n",
    "        score=score,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_feedback(\"217b483c-e520-498d-bb49-d0e0b5a43ff0\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [True, False]\n",
    "sum(list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
