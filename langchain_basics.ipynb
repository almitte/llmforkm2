{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI \n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLMs (Large Language Models) sind leistungsstarke künstliche Intelligenzmodelle, die auf großen Mengen an Textdaten trainiert werden, um natürliche Sprache zu verstehen und zu generieren. Sie können komplexe Aufgaben wie Übersetzung, Zusammenfassung und Antwortgenerierung durchführen und haben eine breite Palette von Anwendungen in der Sprachverarbeitung und anderen Bereichen. LLMs wie GPT-3 von OpenAI oder BERT von Google haben das Potenzial, die Art und Weise zu verändern, wie wir mit Computern interagieren und neue Möglichkeiten für die Automatisierung von Aufgaben zu schaffen.', response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 18, 'total_tokens': 169}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-84e6034a-1ac6-4017-ba5a-e5765b6a05b4-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Erkläre LLMs in drei Sätzen\")\n",
    "# returns JSON data with model output and metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM steht für Master of Laws und ist ein postgraduales Studium für Juristen, die ihre Kenntnisse in einem bestimmten Rechtsgebiet vertiefen möchten. Es wird in der Regel nach dem Abschluss eines Jurastudiums absolviert und dauert meist ein Jahr. Durch ein LLM-Programm können Juristen ihre Karrierechancen verbessern und sich auf spezifische berufliche Herausforderungen vorbereiten.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"Erkläre LLMs in drei Sätzen\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs sind Large Language Models, die auf künstlicher Intelligenz basieren und komplexe Sprachmuster analysieren können. Sie werden häufig für Textgenerierung, Übersetzung und andere sprachbezogene Aufgaben eingesetzt. LLMs wie GPT-3 von OpenAI sind bekannt für ihre Fähigkeit, menschenähnliche Texte zu erstellen und haben das Potenzial, viele Bereiche des menschlichen Lebens zu beeinflussen.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser() # Initialisierung eines Objekts zum Extrahieren des Strings \n",
    "chain = model | parser # Output des Model wird in parser Objekt gepiped\n",
    "chain.invoke(\"Erkläre LLMs in drei Sätzen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Beantworte die Frage basierend auf dem unten gegebenen Kontext. \n",
    "Wenn das Beantworten der Frage nicht möglich ist durch den gegebenen Kontekt, antworte \"Ich weiß es nicht\". \n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template) # Template Prompt Objekt erzeugen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich weiß es nicht.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt.format(context=\"Es gibt auschließlich weiße Schwäne.\", question = \"Gibt es schwarze Schwäne?\")).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basierend auf dem gegebenen Kontext gibt es keine schwarzen Schwäne.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Es gibt ausschließlich weiße Schwäne.\", \n",
    "    \"question\": \"Gibt es schwarze Schwäne?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sì, in Australia ci sono cigni neri.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "diff_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Übersetze {answer} in {language}\"\n",
    ")\n",
    "\n",
    "diff_chain = (\n",
    "    # dic um Input zu definieren, dies wird dann in diff_prompt gepiped\n",
    "    {\"answer\": chain, # Antwort wird aus der anderen Chain als Input genommen\n",
    "     \"language\": itemgetter(\"language\")} # Sprache aus Dictonary\n",
    "    | diff_prompt | model | parser \n",
    ")\n",
    "\n",
    "diff_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Alle Schwäne sind weiß, außer in Australien. Dort gibt es schwarze.\",\n",
    "        \"question\": \"Gibt es schwarze Schwäne?\", \n",
    "        \"language\": \"Italienisch\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[0.00901121263534296, -0.03669295798076368, -0.00873078020641617, -0.025039455366582045, -0.01990443465229751, 0.006879930087054519, -0.0007376917735520665, -0.012351468412199479, -0.009098458010849193, -0.04162856202435205]\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Gibt es schwarze Schwäne?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\") # Dimension der Embedding Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = embeddings.embed_query(\"Es gibt in Europa nur weiße Schwäne\")\n",
    "sen2 = embeddings.embed_query(\"Es gibt schwarze Schwäne in Australien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8740940461805541, 0.9229002354973087)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sen1_sim = cosine_similarity([embedded_query], [sen1])[0][0]\n",
    "query_sen2_sim = cosine_similarity([embedded_query], [sen2])[0][0]\n",
    "query_sen1_sim, query_sen2_sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
